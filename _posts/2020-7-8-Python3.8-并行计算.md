---
layout:     post
title:      python3.8 并行计算
subtitle:   Python 多线程
date:       2020-7-8
author:     ErYueSheng
header-img: img/post-sample-image.jpg
catalog:    true
tags:
    - python
    - 多线程
    - 多进程
    - 并行计算
---

并行计算就是同时完成多件事情，只有再多核或分布式计算机中才能完成真正的并行计算。对于单核CPU计算机，一般通过时间片的方式来“同时”完成很多任务。

进程就是一个大任务，一个进程由很多线程（至少一个）组成，多个线程往往为了配合完成一个任务。线程是操作系统执行的最小单元，也是直接支持的执行单元。

## 多进程

#### multiprocessing

Python提供了跨平台的多进程支持，multiprocessing模块就是Python多进程模块。multiprocessing模块提供一个Process类表示一个进程对象。

    
    from multiprocessing import Process
    import os

    # 子进程要执行的代码
    def run_proc(name):
        print('Run child process %s (%s)...' % (name, os.getpid()))

    if __name__=='__main__':
        print('Parent process %s.' % os.getpid())
        p = Process(target = run_proc, args = ('test',))
        print('Child process will start.')
        p.start()
        p.join()
        print('Child process end.')

通过初始化可以生成一个Process对象，target参数传入要运行的函数，args这时函数的参数表（注意是个**tuple**），Process对象.start()启动子进程。Process.join()等待子进程运行完成，通常用于进程间同步。

#### 进程池pool

要创建大量子进程，可以用进程池的方式批量创建。

    from multiprocessing import Pool
    import os, time, random

    def long_time_task(name):
        print('Run task %s (%s)...' % (name, os.getpid()))
        start = time.time()
        time.sleep(random.random() * 3)
        end = time.time()
        print('Task %s runs %0.2f seconds.' % (name, (end - start)))

    if __name__=='__main__':
        print('Parent process %s.' % os.getpid())
        p = Pool(4)# 后面的参数表示创建几个子进程
        for i in range(5):
            p.apply_async(long_time_task, args=(i,))
        print('Waiting for all subprocesses done...')
        p.close()
        p.join()
        print('All subprocesses done.')

apply_async()函数用于对池中所有子进程分配任务，join()函数需要等所有子进程都结束，再join之前需要调用close()，关闭进程池，不再继续添加新的子进程。

#### 子进程

Python提供subprocess模块可以更加方便的启动子进程，并控制其输入输出。

    import subprocess

    print('$ nslookup www.python.org')
    r = subprocess.call(['nslookup', 'www.python.org'])
    print('Exit code:', r)

还可以使用communicate()方法输入

    import subprocess
    print('$ nslookup')
    p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
    print(output.decode('utf-8'))
    print('Exit code:', p.returncode)

#### 进程间通信

进程之间一般需要相互通信传递数据，multiprocessing模块也包装了很多底层代码，提供了多种方式来交换数据，如Queue、Pipes。
以Queue为例，创建两个子进程，一个往Queue写数据，一个从Queue读数据。

    from multiprocessing import Process, Queue
    import os, time, random

    # 写数据进程执行的代码:
    def write(q):
        print('Process to write: %s' % os.getpid())
        for value in ['A', 'B', 'C']:
            print('Put %s to queue...' % value)
            q.put(value)
            time.sleep(random.random())

    # 读数据进程执行的代码:
    def read(q):
        print('Process to read: %s' % os.getpid())
        while True:
            value = q.get(True)
            print('Get %s from queue.' % value)

    if __name__=='__main__':
        # 父进程创建Queue，并传给各个子进程：
        q = Queue()
        pw = Process(target=write, args=(q,))
        pr = Process(target=read, args=(q,))
        # 启动子进程pw，写入:
        pw.start()
        # 启动子进程pr，读取:
        pr.start()
        # 等待pw结束:
        pw.join()
        # pr进程里是死循环，无法等待其结束，只能强行终止:
        pr.terminate()

所有进程之间的通信，传递的对象，都是通过序列化之后再进行传输的。

## 多线程

Python中的线程是真正的Posix Thread，不是模拟出来的线程。

Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。

#### 创建、启动线程

创建和启动线程和创建启动进程类似，初始化创建实例对象，然后调用start()函数。

    import time, threading

    # 新线程执行的代码:
    def loop():
        print('thread %s is running...' % threading.current_thread().name)
        n = 0
        while n < 5:
            n = n + 1
            print('thread %s >>> %s' % (threading.current_thread().name, n))
            time.sleep(1)
        print('thread %s ended.' % threading.current_thread().name)

    print('thread %s is running...' % threading.current_thread().name)
    t = threading.Thread(target=loop, name='LoopThread')
    t.start()
    t.join()
    print('thread %s ended.' % threading.current_thread().name)

threading模块的current_thread()函数返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2……

#### 线程锁 Lock

多线程和多进程最大的不同在于线程共享内存。多进程中，每个进程都是一个任务，有自己的内存空间和变量。但是多线程中，每个线程访问的都是同一个内存空间、同一个变量。多线程就可能会发生由于数据访问引起的一系列错误，或者更严重的发生死锁、将整个程序锁死。

    import time, threading

    # 假定这是你的银行存款:
    balance = 0

    def change_it(n):
        # 先存后取，结果应该为0:
        global balance
        balance = balance + n
        balance = balance - n

    def run_thread(n):
        for i in range(1000000):
            change_it(n)

    t1 = threading.Thread(target=run_thread, args=(5,))
    t2 = threading.Thread(target=run_thread, args=(8,))
    t1.start()
    t2.start()
    t1.join()
    t2.join()
    print(balance)

最后得到的结果可能并不是0，原因在于chang_it(n)翻译成机器指令去运算会有很多步，如果多个线程同时访问变量，可能正好已经要访问某个值时，这个值刚好被改了，或者之前提取了这个值，发现之后再赋值回去的时候，值发生了改变。这可能就会导致错误发生。

必须保证每个线程再访问共享内存变量时，没有别的线程也在访问这个变量。就是给这个函数（函数访问内存变量）上一把锁，保证只有一个访问变量函在运行，这样就不会有多个线程同时访问共享变量。线程锁Lock可以通过threading.Lock()获得

    lock = threading.Lock()
    def run_thread(n):
        for i in range(100000):
            # 先要获取锁:
            lock.acquire()
            try:
                # 放心地改吧:
                change_it(n)
            finally:
                # 改完了一定要释放锁:
                lock.release()

上锁之后，同时最多只有一个线程能得到Lock，只有有锁的线程才能执行后续的代码，别的没锁的线程只能等待锁被释放，然后由系统再次分配锁给某个线程。一个线程可以拿着锁不放，因为没人规定它一定要释放锁，但是如果一个线程运行后续代码之后不释放锁，则其他的线程都不能获得锁，不能执行后续代码，陷入无尽的等待。另外如果持有锁的线程在运行后续代码的时候发生了错误，导致线程终止，线程结束，但是其锁不会被系统自动释放，只能让别的线程一直干等着，所以上锁后一定要使用try...finally...来保证一定会把锁释放出去。

#### 多核CPU

Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。

## TheadLocal

在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。

ThreadLocal表示一个全局变量，但是可以在这上面定义每个线各自拥有的私有变量属性，每个线程访问该属性会得到各自的私有变量。

    import threading
        
    # 创建全局ThreadLocal对象:
    local_school = threading.local()

    def process_student():
        # 获取当前线程关联的student:
        std = local_school.student
        print('Hello, %s (in %s)' % (std, threading.current_thread().name))

    def process_thread(name):
        # 绑定ThreadLocal的student:
        local_school.student = name
        process_student()

    t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
    t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
    t1.start()
    t2.start()
    t1.join()
    t2.join()

可以将ThreadLocal理解为一个容器，里面包含各个线程的室友变量值，通过线程PID区分所属，当一个线程访问时，就会访问到自己的变量值。

>20200708

## 分布式多进程

在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。

Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。

举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？

原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。

我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务：

    # task_master.py

    import random, time, queue
    from multiprocessing.managers import BaseManager

    # 发送任务的队列:
    task_queue = queue.Queue()
    # 接收结果的队列:
    result_queue = queue.Queue()

    # 从BaseManager继承的QueueManager:
    class QueueManager(BaseManager):
        pass

    # 把两个Queue都注册到网络上, callable参数关联了Queue对象:
    QueueManager.register('get_task_queue', callable=lambda: task_queue)
    QueueManager.register('get_result_queue', callable=lambda: result_queue)
    # 绑定端口5000, 设置验证码'abc':
    manager = QueueManager(address=('', 5000), authkey=b'abc')
    # 启动Queue:
    manager.start()
    # 获得通过网络访问的Queue对象:
    task = manager.get_task_queue()
    result = manager.get_result_queue()
    # 放几个任务进去:
    for i in range(10):
        n = random.randint(0, 10000)
        print('Put task %d...' % n)
        task.put(n)
    # 从result队列读取结果:
    print('Try get results...')
    for i in range(10):
        r = result.get(timeout=10)
        print('Result: %s' % r)
    # 关闭:
    manager.shutdown()
    print('master exit.')

请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。

然后，在另一台机器上启动任务进程（本机上启动也可以）：

    # task_worker.py

    import time, sys, queue
    from multiprocessing.managers import BaseManager

    # 创建类似的QueueManager:
    class QueueManager(BaseManager):
        pass

    # 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
    QueueManager.register('get_task_queue')
    QueueManager.register('get_result_queue')

    # 连接到服务器，也就是运行task_master.py的机器:
    server_addr = '127.0.0.1'
    print('Connect to server %s...' % server_addr)
    # 端口和验证码注意保持与task_master.py设置的完全一致:
    m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
    # 从网络连接:
    m.connect()
    # 获取Queue的对象:
    task = m.get_task_queue()
    result = m.get_result_queue()
    # 从task队列取任务,并把结果写入result队列:
    for i in range(10):
        try:
            n = task.get(timeout=1)
            print('run task %d * %d...' % (n, n))
            r = '%d * %d = %d' % (n, n, n*n)
            time.sleep(1)
            result.put(r)
        except Queue.Empty:
            print('task queue is empty.')
    # 处理结束:
    print('worker exit.')

任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。

现在，可以试试分布式进程的工作效果了。先启动task_master.py服务进程：

    $ python3 task_master.py 
    Put task 3411...
    Put task 1605...
    Put task 1398...
    Put task 4729...
    Put task 5300...
    Put task 7471...
    Put task 68...
    Put task 4219...
    Put task 339...
    Put task 7866...
    Try get results...

task_master.py进程发送完任务后，开始等待result队列的结果。现在启动task_worker.py进程：

    $ python3 task_worker.py
    Connect to server 127.0.0.1...
    run task 3411 * 3411...
    run task 1605 * 1605...
    run task 1398 * 1398...
    run task 4729 * 4729...
    run task 5300 * 5300...
    run task 7471 * 7471...
    run task 68 * 68...
    run task 4219 * 4219...
    run task 339 * 339...
    run task 7866 * 7866...
    worker exit.

task_worker.py进程结束，在task_master.py进程中会继续打印出结果：

    Result: 3411 * 3411 = 11634921
    Result: 1605 * 1605 = 2576025
    Result: 1398 * 1398 = 1954404
    Result: 4729 * 4729 = 22363441
    Result: 5300 * 5300 = 28090000
    Result: 7471 * 7471 = 55815841
    Result: 68 * 68 = 4624
    Result: 4219 * 4219 = 17799961
    Result: 339 * 339 = 114921
    Result: 7866 * 7866 = 61873956

这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。

Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中：

![UnyytJ.png](https://s1.ax1x.com/2020/07/09/UnyytJ.png)

而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。

authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。

>20200709
